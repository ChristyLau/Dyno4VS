{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0901c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.signal import argrelmax, argrelmin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import MDAnalysis as mda\n",
    "\n",
    "import ipynb_importer\n",
    "import visualize, parsers\n",
    "\n",
    "\n",
    "def add_distance_mat(data, dyno_dict, include_time = False):\n",
    "    max_frame = max([x[\"frames\"][-1] for x in data.values()])\n",
    "    \n",
    "    for key in dyno_dict.keys():\n",
    "        data_tmp = data[key][\"points\"]\n",
    "        if len(data_tmp.shape) == 1:\n",
    "            data_tmp.reshape(-1, 1)\n",
    "        distances = pairwise_distances(data_tmp)\n",
    "        \n",
    "        # add time component in distance matrix\n",
    "        if include_time:\n",
    "            frames_norm = (np.sqrt(data[key][\"frames\"])/max_frame).reshape(-1,1)  # why sqrt: flexible conformation shows displacement within local time, not through the whole trajectory\n",
    "            frames_distance = pairwise_distances(frames_norm)\n",
    "            distances = distances + frames_distance\n",
    "            distances = distances/np.max(distances)\n",
    "\n",
    "        data[key][\"distances\"] = distances\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_auto_param(data, dyno_dict, include_time = False, info_table = True, \n",
    "                   frequency_cutoff = 0.06, plot = True):\n",
    "    '''Preliminary prediction on radius_cutoff and cnn_cutoff\n",
    "       based on data distribution\n",
    "       \n",
    "       input: dict\n",
    "       output: dict with new key\n",
    "           [\"params\"] = {\n",
    "                        \"radius_cutoff\": radius,\n",
    "                        \"cnn_cutoff\": cnn_cutoff,\n",
    "                        \"member_cutoff\": member_cutoff\n",
    "                    }\n",
    "       '''\n",
    "    default_cluster_params = {\n",
    "        \"radius_cutoff\": 0.2,\n",
    "        \"cnn_cutoff\": 5,\n",
    "        \"member_cutoff\": 10\n",
    "    }\n",
    "    \n",
    "    for i, key in enumerate(dyno_dict.keys()):\n",
    "        if not include_time:\n",
    "            data_tmp = data[key][\"points\"]\n",
    "            distances = pairwise_distances(data_tmp)\n",
    "            radius_multi = 0.8\n",
    "        else:\n",
    "            '''include time'''\n",
    "            distances = data[key][\"distances\"]\n",
    "            radius_multi = 0.6\n",
    "\n",
    "        min_n_cluster = 0\n",
    "        min_freq = 0\n",
    "        frame_nr = len(distances[0])\n",
    "        max_frame = max([x[\"frames\"][-1] for x in data.values()])\n",
    "        radius = 0\n",
    "        \n",
    "        weights = np.zeros_like(distances.flatten()) + 1. / distances.flatten().size\n",
    "        y, x, _ = plt.hist(distances.flatten(), bins=100, color='y', weights=weights)\n",
    "        plt.close()\n",
    "\n",
    "        if frame_nr >= frequency_cutoff*max_frame:  # TODO: depends on frequency\n",
    "            n, bins = np.histogram(distances, 20)\n",
    "\n",
    "            # get the peaks and valleys' position\n",
    "            x_left = np.min(distances)\n",
    "            x_right = np.max(distances)\n",
    "            interval = (x_right - x_left)/20\n",
    "\n",
    "            # predict min cluster number based on distance distribution\n",
    "            n_cluster = len(argrelmax(n)[0])\n",
    "            \n",
    "            if n_cluster <= 4: # cluster > 4 could resulted from noice data, so not predicted\n",
    "                # get minimal frequency at valleys\n",
    "                valley_pos = [i*5 for i in argrelmin(n)]\n",
    "\n",
    "                if len(y[tuple(valley_pos)]):\n",
    "                    min_freq = round(min(y[tuple(valley_pos)]), 3)\n",
    "                    max_freq = round(np.max(y), 3)\n",
    "\n",
    "                min_n_cluster = n_cluster\n",
    "\n",
    "                max_ = (np.array(argrelmax(n))*interval)[0]\n",
    "                min_ = (np.array(argrelmin(n))*interval)[0]\n",
    "                min_ = np.insert(min_, 0, 0)\n",
    "\n",
    "                # predict radius based on average peak width/2\n",
    "                min_len = min(len(max_), len(min_))\n",
    "                radius = round(np.mean(max_[:min_len] - min_[:min_len])*radius_multi, 3)\n",
    "\n",
    "                # predict cnn_cutoff based on points number: 5%*points number\n",
    "                if min_freq != 0:\n",
    "                    cnn_cutoff_ = round(min(frame_nr*min_freq, frame_nr*0.05, 15))\n",
    "                else:\n",
    "                    cnn_cutoff_ = round(min(frame_nr*0.05, 15))\n",
    "\n",
    "                data[key][\"params\"] = {\n",
    "                    \"radius_cutoff\": radius,\n",
    "                    \"cnn_cutoff\": cnn_cutoff_,\n",
    "                    \"member_cutoff\": 10\n",
    "                }\n",
    "        else:\n",
    "            data[key][\"params\"] = default_cluster_params\n",
    "\n",
    "        data[key][\"min_cluster_n\"] = min_n_cluster\n",
    "        \n",
    "        if plot:\n",
    "            value = data[key][\"params\"][\"radius_cutoff\"]\n",
    "            plt.figure(figsize=(4,2))\n",
    "            plt.hist(distances.flatten(), bins=100, color='y')\n",
    "            plt.axvline(value)\n",
    "\n",
    "            plt.title(f\"{i}: {key}\", fontsize = 6)\n",
    "            plt.annotate(f\"r = {round(value, 2)}\", (0.05, 0.95), xycoords=\"axes fraction\", fontsize=6)\n",
    "            plt.show()\n",
    "        # print infor table\n",
    "        if info_table:\n",
    "            info_table = [min_freq, min_n_cluster, radius, cnn_cutoff_]\n",
    "            print(i, key)\n",
    "            print (\"{:<15} {:<15} {:<8} {:<15}\".format('min_frequency','min_cluster','radius','cnn_cutoff')) # ti\n",
    "            print (\"{:<15} {:<15} {:<8} {:<15}\".format(min_freq, min_n_cluster, radius, cnn_cutoff_))\n",
    "            print(\"-\"*50)\n",
    "        \n",
    "    return data\n",
    "\n",
    "def get_state_matrix(data):\n",
    "    '''wrap up state information of all superfeatures into one ndarray\n",
    "       output: ndarray of shape (max_frame_nr, superfature_count)\n",
    "       '''\n",
    "    state_matrix = []\n",
    "    max_frame = max([x[\"frames\"][-1] for x in data.values()])\n",
    "\n",
    "    for fkey in data.keys():\n",
    "        current_frame = 0\n",
    "        state_matrix.append([])\n",
    "        padded = state_matrix[-1]\n",
    "        for frame_id, clabel in zip(data[fkey][\"frames\"], data[fkey][\"clustering\"].labels):\n",
    "\n",
    "            while frame_id > current_frame:\n",
    "                padded.append(0)\n",
    "                current_frame += 1\n",
    "            padded.append(clabel)\n",
    "            current_frame += 1\n",
    "        while current_frame <= max_frame:\n",
    "            padded.append(0)\n",
    "            current_frame += 1\n",
    "            \n",
    "    state_matrix = np.asarray(state_matrix).T\n",
    "        \n",
    "    return state_matrix\n",
    "\n",
    "\n",
    "def get_one_hot_encoding(state_matrix):\n",
    "    '''Transform state_matrix into one hot key matrix\n",
    "       output: ndarray\n",
    "           frame | existance of state 0 in superfeature 1 | existance of state 1 in superfeature 1...\n",
    "           1     | 1 (means exist)                        | 0 (means absence)\n",
    "           2     ...\n",
    "           3     ...\n",
    "    '''\n",
    "    encoder = OneHotEncoder(sparse = False)\n",
    "    one_hot_matrix = encoder.fit_transform(state_matrix)\n",
    "    return one_hot_matrix\n",
    "\n",
    "\n",
    "def get_frames_each_cluster(pam, shift = 0):\n",
    "    '''input: KMedoids object\n",
    "       output: dict\n",
    "           {binding state 1: ndarray of frames,\n",
    "            binding state 1: ...}'''\n",
    "    state_nr = np.max(pam.labels_) + 1\n",
    "    cluster_frames_map = {\n",
    "        k: np.where(pam.labels_ == k)[0]\n",
    "        for k in range(state_nr)\n",
    "    }\n",
    "    return cluster_frames_map\n",
    "\n",
    "\n",
    "def get_state_statistis(pam, data, dynophore_dict):\n",
    "    '''Get frame count of each superfeature state for each binding poses\n",
    "       output: dict\n",
    "           e.g. {binding pose 1: {superfeature 1:{feature state 0: count of frames, feature state 1: count of frames},\n",
    "                                 {superfeature 2:{feature state 0: count of frames},\n",
    "                                 {superfeature 3...}}}'''\n",
    "    state_matrix = get_state_matrix(data)\n",
    "    cluster_frames_map = get_frames_each_cluster(pam)\n",
    "    state_statistis = {}\n",
    "\n",
    "    for state_idx, frames in cluster_frames_map.items():\n",
    "        state_data = {}\n",
    "        data_per_state = state_matrix[frames]\n",
    "        for feature_idx, feature in enumerate(dynophore_dict.keys()):\n",
    "            data_per_feature_state = data_per_state[:, feature_idx]\n",
    "            stata_feature_data = {}\n",
    "            for cluster in np.unique(data_per_feature_state):\n",
    "                cluster_count = len(data_per_feature_state[data_per_feature_state == cluster])\n",
    "                stata_feature_data[cluster] = cluster_count\n",
    "            state_data[feature_idx] = stata_feature_data\n",
    "\n",
    "        state_statistis[state_idx] = state_data\n",
    "        \n",
    "    return state_statistis\n",
    "    \n",
    "    \n",
    "def get_feature_freq_per_state(state_statistis, data):\n",
    "    '''Get frequency of each superfeature in each binding pose\n",
    "       output: dict\n",
    "           e.g. {binding pose 1: {superfeature 1:frequency,\n",
    "                                 {superfeature 2:frequency,\n",
    "                                 {superfeature 3...}}}'''\n",
    "    max_frame = max([x[\"frames\"][-1] for x in data.values()])\n",
    "    \n",
    "    feature_per_state = {}\n",
    "    for state_idx, data_ in state_statistis.items():\n",
    "        cache = {}\n",
    "        for feature, _data_ in data_.items():\n",
    "            number = 0\n",
    "            for state, count in _data_.items():\n",
    "                if state > 0:\n",
    "                    number += count\n",
    "            cache[feature] = number/max_frame\n",
    "        feature_per_state[state_idx] = cache\n",
    "    return feature_per_state\n",
    "\n",
    "\n",
    "def get_interact_summary(data, pam, dynophore_dict):\n",
    "    '''output: DataFrame\n",
    "       e.g.\n",
    "                0\t1\t2\t3\tfeature\n",
    "    0\t0.077521\t0.130609\t0.067970\t0.290982\tH[3187,3181,3178,3179,3185,3183]\n",
    "    1\t0.000000\t0.000000\t0.000000\t0.000000\tH[3146]\n",
    "    2\t0.249000\t0.117503\t0.166148\t0.432474\tH[3150,3152,3158,3156,3154,3149]'''\n",
    "    state_matrix = get_state_matrix(data)\n",
    "    state_statistis = get_state_statistis(pam, data, dynophore_dict)\n",
    "    feature_per_state = get_feature_freq_per_state(state_statistis, data)\n",
    "    \n",
    "    feature_per_state_df = pd.DataFrame(feature_per_state)\n",
    "    interact_summary = feature_per_state_df.copy()\n",
    "    interact_summary[\"feature\"] = dynophore_dict.keys()\n",
    "    return interact_summary\n",
    "\n",
    "\n",
    "def reduce_frames(pdb_path, dcd_path, select = \"protein or chainID X\", out_path = f\"output/reduced.dcd\", final_n_frame = 500):\n",
    "    '''write reduce trajectory to desired length in output folder'''\n",
    "    u = mda.Universe(pdb_path, dcd_path)\n",
    "    frames = [i for i in range(0, len(u.trajectory), len(u.trajectory)//final_n_frame)]\n",
    "    n_atoms = int(str(u)[-11:-7])\n",
    "    \n",
    "    with mda.Writer(out_path, n_atoms = n_atoms) as W:\n",
    "        for ts in u.trajectory[frames]:\n",
    "            W.write(u.select_atoms(select))\n",
    "            \n",
    "            \n",
    "def get_geo_center(data):\n",
    "    x, y, z = (max(data[:, 0]) + min(data[:, 0]))/2, (max(data[:, 1]) + min(data[:, 1]))/2, (max(data[:, 2]) + min(data[:, 2]))/2\n",
    "    return (x, y, z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
